{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "births_subgroup_definitions = {\n",
    "        'race': (\"nhwhite\", \"hisp\", \"nhblack\", \"otherraceeth\"),\n",
    "        'edu': (\"nohs\", \"hs\", \"somecoll\", \"coll\"),\n",
    "       # 'edu': (\"hs_less\", \"somecoll_more\"),\n",
    "        #'age': (\"age1519\", \"age2024\", \"age2529\", \"age3034\", \"age3539\", \"age4044\"),\n",
    "        'age': (\"age1524\", \"age2534\", \"age3544\"),\n",
    "        'insurance': (\"medicaid\", \"nonmedicaid\"),\n",
    "        # California should be dropped for marital. \n",
    "        'marital': (\"married\", \"unmarried\"),\n",
    "        'total': (\"total\",),\n",
    "    }\n",
    "deaths_subgroup_definitions = {\n",
    "        'race': (\"nhwhite\", \"hisp\", \"nhblack\", \"otherraceeth\"),\n",
    "        'neonatal' : (\"neo\", \"nonneo\"),\n",
    "        'congenital' : (\"con\", \"noncon\"),\n",
    "        'total': (\"total\",),\n",
    "    }\n",
    "\n",
    "deaths_residual_category_definitions = {\n",
    "    'race'  : \"otherraceeth\",\n",
    "    'neonatal' : \"nonneo\",\n",
    "    'congenital' : \"noncon\",\n",
    "    'total' : None\n",
    "}\n",
    "\n",
    "subgroup_definitions = {\n",
    "    'births': births_subgroup_definitions,\n",
    "    'deaths': deaths_subgroup_definitions\n",
    "}\n",
    "\n",
    "def clean_dataframe(dat:pd.DataFrame, outcome_type=\"births\", cat_name=\"total\", \n",
    "                    csv_filename='data/dat_quarterly.csv', end_date='2024-01-1',\n",
    "                    dobbs_donor_sensitivity=False):\n",
    "    \"\"\"\n",
    "    Filters, imputes, and adds relevant columns to the dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    # Set time variable for bimonths\n",
    "    dat['time'] = pd.to_datetime(dat.year.astype(str) + '-' + (dat.bacode * 6 - 5).astype(str) + \"-01\")\n",
    "\n",
    "    dat['deaths_nonneo'] = dat['deaths_total'] - dat['deaths_neo']\n",
    "    #dat['deaths_noncon'] = dat['deaths_total'] - dat['deaths_con']\n",
    "    dat['births_con'] = dat['births_noncon'] = dat['births_total']\n",
    "    dat['births_neo'] = dat['births_nonneo'] = dat['births_total']\n",
    "    \n",
    "    # Convert \"births_nhblack\" column to numeric, replacing \"Suppressed\" with NaN\n",
    "    dat['births_nhblack'] = pd.to_numeric(dat['births_nhblack'].replace(\"Suppressed\", pd.NA))\n",
    "\n",
    "    # Create a new column 'partial_ban' based on conditions from 'dobbscodev3' column\n",
    "    dat['partial_ban'] = dat['dobbscodev3'].apply(lambda x: 1 if x == 2 else 0)\n",
    "    #dat['time'] = pd.to_datetime(dat.year.astype(str) + '-' + dat.month, format=\"%Y-%B\")\n",
    "\n",
    "    def fill_in_missing_denoms(dat):\n",
    "        # Get a list of column names containing the string \"pop\"\n",
    "        cols_with_pop = dat.filter(regex=r'pop').columns\n",
    "        # Iterate over each column containing \"pop\"\n",
    "        for col in cols_with_pop:\n",
    "            # Find the row index of the maximum value for the current column in 2022\n",
    "            pop_index_2022 = dat.loc[dat['year'] == 2022, col].idxmax()\n",
    "            # Find the row index of the maximum value for the current column in 2021\n",
    "            pop_index_2021 = dat.loc[dat['year'] == 2021, col].idxmax()\n",
    "            \n",
    "            # Impute missing values in the current column\n",
    "            # For rows with missing values, replace with the squared value of the 2022 row divided by the 2021 row\n",
    "            dat.loc[dat[col].isna(), col] = (dat.loc[pop_index_2022, col] ** 2) / dat.loc[pop_index_2021, col]\n",
    "        \n",
    "        # Return the modified DataFrame\n",
    "        return dat\n",
    "\n",
    "    # Hacky imputation\n",
    "    # All of 2023 currently has a population of `NA`\n",
    "    # We'll use a linear imputation of (pop 2022) * (pop 2022) / (pop 2021)\n",
    "    # Well do so by:\n",
    "    ## Group the original DataFrame by 'state'\n",
    "    ## Apply the fill_in_missing_denoms function to each group\n",
    "    ## Ungroup the result and reset the index\n",
    "    dat = dat.groupby('state').apply(fill_in_missing_denoms).reset_index(drop=True)\n",
    "\n",
    "    # create a column that is YYYY-QQ for indexing later\n",
    "    #dat = dat.assign(time=dat['year'].astype(str) + '-' + ((dat['q'] - 1) * 3 + 1).astype(str))\n",
    "    dat['quarter'] = dat['time'].apply(lambda x: f\"{pd.Period(x, freq='Q').start_time.year}-Q{pd.Period(x, freq='Q').quarter}\")\n",
    "\n",
    "\n",
    "    ## Correct for different number of days each month\n",
    "\n",
    "    # Assuming 'dat' is a DataFrame in Python\n",
    "    # Convert 'time' column to datetime if it's not already\n",
    "    dat['time'] = pd.to_datetime(dat['time'])\n",
    "\n",
    "    # Filter the DataFrame to keep rows with time before end_date\n",
    "    dat = dat[dat['time'] < pd.to_datetime(end_date)]\n",
    "\n",
    "    ## States where abortion was banned in July 2022 include Alabama, Arkansas, Mississippi, Missouri, Oklahoma, South Dakota, \n",
    "    # Texas, West Virginia, and Wisconsin. Kentucky and Louisiana banned abortion in August and Idaho and Tennessee \n",
    "    # did so in September 2022. North Dakota banned abortion in April 2023. Ohio and South Carolina had six-week bans in place \n",
    "    # for the first two months after the Dobbs decision and Georgia had a six-week ban beginning in August 2022.  \n",
    "    # States where abortion was banned in July 2022\n",
    "    # july_banned_states = ['Alabama', 'Arkansas', 'Mississippi', 'Missouri', 'Oklahoma', 'South Dakota', 'Texas', 'West Virginia', 'Wisconsin']\n",
    "\n",
    "    # # States where abortion was banned in August 2022\n",
    "    # august_banned_states = ['Kentucky', 'Louisiana']\n",
    "\n",
    "    # # States where abortion was banned in September 2022\n",
    "    # september_banned_states = ['Idaho', 'Tennessee']\n",
    "\n",
    "    # # States where abortion was banned in April 2023\n",
    "    # april_banned_states = ['North Dakota']\n",
    "\n",
    "    # # States with six-week bans in place for the first two months after the Dobbs decision\n",
    "    # six_week_ban_states = ['Ohio', 'South Carolina']\n",
    "\n",
    "    # # Georgia had a six-week ban beginning in August 2022\n",
    "    # georgia_six_week_ban = ['Georgia']\n",
    "\n",
    "    # # Combine all banned states\n",
    "    # banned_states = (\n",
    "    #     july_banned_states +\n",
    "    #     august_banned_states +\n",
    "    #     september_banned_states +\n",
    "    #     april_banned_states +\n",
    "    #     six_week_ban_states +\n",
    "    #     georgia_six_week_ban\n",
    "    # )\n",
    "\n",
    "\n",
    "    # # Assign dobbscodev2 to dobbs_code\n",
    "    # dat['dobbscodev2'] = dat.groupby('state')['dobbscodev2'].ffill()\n",
    "    \n",
    "    # dat['dobbs_code'] = dat['dobbscodev2']\n",
    "\n",
    "    # # Set dobbs_code to 0 for Texas for times < 2022-04-01\n",
    "    # dat.loc[(dat['state'] == 'Texas') & (dat['time'] < pd.to_datetime('2022-04-01')), 'dobbs_code'] = 0\n",
    "\n",
    "    # # Set dobbs_code to 0 for all other states for times < 2023-01-01\n",
    "    # dat.loc[(dat['state'] != 'Texas') & (dat['time'] < pd.to_datetime('2023-01-01')), 'dobbs_code'] = 0\n",
    "\n",
    "    # # Set dobbs_code to 0 for Kentucky and Louisiana for times before 2023-02-01\n",
    "    # dat.loc[(dat['state'].isin(['Kentucky', 'Louisiana'])) & (dat['time'] < pd.to_datetime('2023-02-01')), 'dobbs_code'] = 0\n",
    "    \n",
    "    # # Set dobbs_code to 0 for Idaho and Tennessee for times before 2023-03-01\n",
    "    # dat.loc[(dat['state'].isin(['Idaho', 'Tennessee'])) & (dat['time'] < pd.to_datetime('2023-03-01')), 'dobbs_code'] = 0\n",
    "\n",
    "    # # Set dobbs_code to 0 for Georgia\n",
    "    # dat.loc[(dat['state'] == 'Georgia'), 'dobbs_code'] = 0\n",
    "    # dat.loc[(dat['state'].isin(['Georgia'])) & (dat['time'] > pd.to_datetime('2023-04-01')), 'dobbs_code'] = 1\n",
    "    \n",
    "\n",
    "\n",
    "        # Create a control index array DataFrame\n",
    "    if outcome_type == \"births\":\n",
    "        dat['exposure_code'] = dat['exposed_births']\n",
    "    if outcome_type == \"deaths\":\n",
    "        dat['exposed_infdeaths'] = dat['exposed_infdeaths'].bfill()\n",
    "        dat['exposure_code'] = dat['exposed_infdeaths']\n",
    "\n",
    "    # Convert to a list of unique states with dobbs_code == 1\n",
    "    \n",
    "    states_with_ban = dat.loc[dat['exposure_code'] == 1, 'state'].unique().tolist()\n",
    "\n",
    "    # Create a new column 'births_other' by subtracting births of non-Hispanic white, Hispanic, and non-Hispanic black from total births\n",
    "    dat['births_other'] = dat['births_total'] - dat['births_nhwhite'] - dat['births_hisp'] - dat['births_nhblack']\n",
    "\n",
    "    dat = dat.sort_values(['state', 'time'])\n",
    "\n",
    "    # Remove California for marital\n",
    "    if cat_name == \"marital\":\n",
    "        dat = dat[dat[\"state\"] != \"California\"]\n",
    "    \n",
    "    if dobbs_donor_sensitivity:\n",
    "        sensitivity_states = dat[~dat[\"dobbscode_sensitivity\"].isna()]['state'].unique()\n",
    "        sensitivity_states = [state for state in sensitivity_states if state not in ['Arizona', 'Pennsylvania', 'Florida', 'California']]\n",
    "        dat = dat[dat[\"state\"].isin(sensitivity_states)]\n",
    "    \n",
    "    \n",
    "    if csv_filename is not None:\n",
    "        ## Save to csv so we don't have to do this every time \n",
    "        dat.to_csv(csv_filename)\n",
    "    return dat\n",
    "\n",
    "\n",
    "def prep_data(dat, group=None, outcome_type=\"births\", variables=None, covariates=None):\n",
    "    \"\"\"\n",
    "    Prepare data for analysis by creating DataFrames for births or deaths (numerators), population or births (denominators), control indices, and missing indices.\n",
    "\n",
    "    Args:\n",
    "        dat (pandas.DataFrame): Input data containing information about births, population, and other relevant variables.\n",
    "        variables (list, optional): List of variable names (e.g., \"white\", \"hisp\", \"black\", \"other\"). Default is [\"white\", \"hisp\", \"black\", \"other\"].\n",
    "        covariates (list, optional): List of covariate names to include in the analysis. Default is None.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the following items:\n",
    "            Y (pandas.DataFrame): DataFrame with birth counts for each category, state, and time period.\n",
    "            population (pandas.DataFrame): DataFrame with population counts for each category, state, and time period.\n",
    "            state_fe (numpy.ndarray): Array of state fixed effects.\n",
    "            control_idx_array (pandas.DataFrame): DataFrame with control indices for each category, state, and time period.\n",
    "            missing_idx_array (pandas.DataFrame): DataFrame with missing indices for each category, state, and time period.\n",
    "            days_multiplier (float): Days multiplier value.\n",
    "            variables (list): List of variable names used in the analysis.\n",
    "            D_cov (numpy.ndarray or None): Matrix of covariates, if provided. If no covariates are provided, D_cov is set to None.\n",
    "    \"\"\"\n",
    "    if (group is not None) and (variables is not None):\n",
    "        raise Exception(\"Only one of group/variables can be specified.\")\n",
    "    if group is not None:\n",
    "        variables = subgroup_definitions[outcome_type][group]\n",
    "    \n",
    "\n",
    "    if outcome_type == \"deaths\":\n",
    "        \n",
    "        # Create a list of death column names\n",
    "        death_columns = [\"deaths_\" + var for var in variables]    \n",
    "        \n",
    "        # Create a deaths DataFrame\n",
    "        deaths = (\n",
    "            dat[[\"state\", \"time\"] + death_columns]  # Select 'state', 'time', and death columns\n",
    "            .melt(id_vars=[\"state\", \"time\"], value_vars=death_columns, var_name=\"category\", value_name=\"deaths\")  # Melt death columns into long format\n",
    "            .pivot_table(index=[\"category\"], columns=[\"state\", \"time\"], values=\"deaths\", aggfunc=\"sum\", fill_value=0)  # Pivot to wide format, summing death values\n",
    "        )\n",
    "\n",
    "    # Create a list of birth column names\n",
    "    birth_columns = [\"births_\" + var for var in variables]\n",
    "    \n",
    "    # Create a list of population column names\n",
    "    denom_columns = [\"pop_\" + var for var in variables]\n",
    "\n",
    "    if outcome_type == \"births\":\n",
    "        # Create a population DataFrame\n",
    "        population = (\n",
    "            dat[[\"state\", \"time\"] + denom_columns]  # Select 'state', 'time', and population columns\n",
    "            .melt(id_vars=[\"state\", \"time\"], value_vars=denom_columns, var_name=\"category\", value_name=\"population\")  # Melt population columns into long format\n",
    "            .pivot_table(index=[\"category\"], columns=[\"state\", \"time\"], values=\"population\", aggfunc=\"sum\", fill_value=0)  # Pivot to wide format, summing population values \n",
    "        ) \n",
    "\n",
    "    # Create a births DataFrame\n",
    "    births = (\n",
    "        dat[[\"state\", \"time\"] + birth_columns]  # Select 'state', 'time', and birth columns\n",
    "        .melt(id_vars=[\"state\", \"time\"], value_vars=birth_columns, var_name=\"category\", value_name=\"births\")  # Melt birth columns into long format\n",
    "        .pivot_table(index=[\"category\"], columns=[\"state\", \"time\"], values=\"births\", aggfunc=\"sum\", fill_value=0)  # Pivot to wide format, summing birth values\n",
    "    )\n",
    "    \n",
    "    if outcome_type == \"deaths\":\n",
    "        Y = deaths\n",
    "        denominators = births\n",
    "        outcome_columns = death_columns\n",
    "    else:\n",
    "        Y = births\n",
    "        denominators = population / 1e4 # Population per 1000\n",
    "        outcome_columns = birth_columns\n",
    "    \n",
    "    num_states = len(dat.state.unique())\n",
    "    total_length = denominators.shape[1]\n",
    "    denominators = denominators.values.reshape((len(variables), num_states, denominators.shape[1]//num_states))\n",
    "    Y = Y.values.reshape((len(variables), num_states, total_length//num_states))\n",
    "\n",
    "    control_idx_array = (\n",
    "        dat[[\"state\", \"time\", \"exposure_code\"] + outcome_columns]  # Select 'state', 'time', 'exposed_births', and birth/death columns\n",
    "        .melt(id_vars=[\"state\", \"time\", \"exposure_code\"], value_vars=outcome_columns, var_name=\"category\", value_name=outcome_type)  # Melt birth columns into long format\n",
    "        .assign(ctrl_index=(lambda x: x[\"exposure_code\"] == 0))  # Create a control index column based on 'exposed_births'\n",
    "        .pivot_table(index=[\"category\"], columns=[\"state\", \"time\"], values=\"ctrl_index\", aggfunc=\"sum\", fill_value=0)  # Pivot to wide format, summing control index values\n",
    "    ).astype(np.bool_) # cast to a boolean so we don't have issues when we mask\n",
    "    \n",
    "    control_idx_array = control_idx_array.values.reshape((len(variables), num_states, total_length//num_states))\n",
    "\n",
    "    # Create a missing index array DataFrame\n",
    "    missing_idx_array = (\n",
    "        dat[[\"state\", \"time\", \"exposure_code\"] + outcome_columns]  # Select 'state', 'time', 'exposure_code', and birth columns\n",
    "        .melt(id_vars=[\"state\", \"time\", \"exposure_code\"], value_vars=outcome_columns, var_name=\"category\", value_name=outcome_type)  # Melt birth columns into long format\n",
    "        .assign(missing_index=lambda x: x[outcome_type].isna().astype(int))  # Create a missing index column based on missing birth values\n",
    "        .pivot_table(index=[\"category\"], columns=[\"state\", \"time\"], values=\"missing_index\", aggfunc=\"sum\", fill_value=0)  # Pivot to wide format, summing missing index values\n",
    "    ).astype(np.bool_) # cast to a boolean so we don't have issues when we mask\n",
    "    \n",
    "    missing_idx_array = missing_idx_array.values.reshape((len(variables), num_states, total_length//num_states))\n",
    "\n",
    "    residual_cat_mask_idx_array = np.zeros_like(control_idx_array)\n",
    "    if group == \"neonatal\": \n",
    "        residual_cat_mask_idx_array[variables.index(deaths_residual_category_definitions[group]), :, :] = 1\n",
    "    residual_cat_mask_idx_array = residual_cat_mask_idx_array.astype(np.bool_)\n",
    "        \n",
    "    # If covariates are provided, calculate the covariates matrix\n",
    "    if covariates is not None:\n",
    "        D_cov = dat.groupby(\"state\")[covariates].mean().reset_index()[covariates].values\n",
    "        D_cov[np.isnan(D_cov)] = D_cov[~np.isnan(D_cov)].mean()\n",
    "    else:\n",
    "        D_cov = None\n",
    "    \n",
    "\n",
    "    # Return a dictionary with the calculated values\n",
    "    return {\n",
    "        \"Y\": Y,\n",
    "        \"denominators\": denominators,\n",
    "        #\"state_fe\": state_fe,\n",
    "        \"control_idx_array\": control_idx_array,\n",
    "        \"missing_idx_array\": missing_idx_array, \n",
    "        \"residual_cat_mask_idx_array\": residual_cat_mask_idx_array,\n",
    "        #\"days_multiplier\": days_multiplier,\n",
    "        \"variables\": variables,\n",
    "        \"D_cov\": D_cov,\n",
    "    }\n",
    "\n",
    "def create_unit_placebo_dataset(df, treated_state = \"Texas\", placebo_state = \"California\"):\n",
    "    \"\"\"\n",
    "    Create a placebo dataset for by giving Texas' treatment times to `placebo_state` state by removing the state \n",
    "    and removing Texas.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): Input data containing information about births, population, exposure codes and other relevant variables.\n",
    "        placebo_state (str): Name of the placebo state to remove from the dataset.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: Placebo dataset with the specified placebo state with treatment times of treated_state.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Creating unit-placebo dataset for {}\".format(placebo_state))\n",
    "\n",
    "    # Get the columns that start with 'exposed' (exposed_births/exposed_deaths)\n",
    "    exposure_columns = df.filter(regex='^exposure').columns\n",
    "\n",
    "    # Get the values from the rows where 'state' equals 'treated_state'\n",
    "    treated_values = df.loc[df['state'] == treated_state, exposure_columns]\n",
    "\n",
    "    # Set the values in the rows where 'state' equals 'placebo_state' to the values from the 'treated_state' rows\n",
    "    df.loc[df['state'] == placebo_state, exposure_columns] = treated_values.values\n",
    "    \n",
    "    # Filter the DataFrame to keep rows where 'state' is not equal to the treated state\n",
    "    return df[df[\"state\"] != treated_state]\n",
    "\n",
    "def create_time_placebo_dataset(df, new_treatment_start=\"2022-05-01\", original_earliest_time = \"2012-01-01\"):\n",
    "    \"\"\"\n",
    "    Create a time placebo dataset by shifting treatment times early and capping the end date.\n",
    "    \n",
    "    Args:\n",
    "        df (pandas.DataFrame): Input data containing information about births, population, exposure codes and other relevant variables.\n",
    "        first_treatment_start (str): Start time of the first treated unit (usually Texas). Set to \"2022-05-01\" by default which is the actual SB8 time.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: Modified dataset with shifted time variables.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Creating placebo-in-time dataset starting in {}\".format(new_treatment_start))\n",
    "    \n",
    "    def round_date_to_nearest_half_year(ts: pd.Timestamp) -> pd.Timestamp:\n",
    "        if 4 <= ts.month <=8:\n",
    "            return pd.Timestamp(ts.year, 7, 1)\n",
    "        elif ts.month >=9:\n",
    "            return pd.Timestamp(ts.year+1, 1, 1)\n",
    "        elif ts.month <= 3:\n",
    "            return pd.Timestamp(ts.year, 1, 1)\n",
    "        else:\n",
    "            raise Exception(\"Logic error.\")\n",
    "\n",
    "    # Convert 'first_treatment_start' to datetime\n",
    "        \n",
    "    new_treatment_start = pd.to_datetime(new_treatment_start)\n",
    "    original_treatment_start = df.loc[df['exposure_code'] == 1, 'time'].min()\n",
    "\n",
    "    end_date = df.loc[df['exposure_code'] == 1, 'time'].max()\n",
    "    \n",
    "    new_end = new_treatment_start + (end_date - original_treatment_start)\n",
    "    \n",
    "    original_time_length = end_date - pd.to_datetime(original_earliest_time)\n",
    "    new_start = (new_end - original_time_length)\n",
    "    if new_start < df[\"time\"].min():\n",
    "        new_start = df[\"time\"].min()\n",
    "    \n",
    "    new_start = round_date_to_nearest_half_year(new_start)\n",
    "    new_end = round_date_to_nearest_half_year(new_end)\n",
    "    \n",
    "    new_time_length = new_end - new_start\n",
    "\n",
    "    # Get the columns that start with 'exposure_code'\n",
    "    exposure_code_values = df.loc[(df['time'] >= end_date - new_time_length), ['exposure_code']]\n",
    "\n",
    "    df = df[(df['time'] <= new_end) & (df['time'] >= new_start)]\n",
    "\n",
    "    if len(exposure_code_values) == len(df):\n",
    "        df.loc[:, \"exposure_code\"] = exposure_code_values.values\n",
    "    else:\n",
    "        raise ValueError(\"The length of new exposure_code values does not match the number of rows in df\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     import argparse\n",
    "#     import pandas as pd\n",
    "#     import pickle \n",
    "#     import os\n",
    "#     import gzip \n",
    "    \n",
    "#     parser = argparse.ArgumentParser(description='Script for cleaning and parsing quarterly births. We will assume that the file name is in the format quarterly_fertility_mortality_MMDDYY.csv where MM is month,DD is day, and YY is year.')\n",
    "#     parser.add_argument('filename', type=str, help='Name of the input file')\n",
    "#     parser.add_argument('--save-dict', action='store_true', help='Create and save the dictionary used for model training as a pkl file')\n",
    "#     parser.add_argument(\"--group\", help=\"Subgroups to create a dictionary for\", default=\"total\")\n",
    "\n",
    "#     args = parser.parse_args()\n",
    "#     # Create a directory to hold the parsed files\n",
    "#     directory_name = 'model_data/' + args.filename.split('/')[-1].split('.')[0]\n",
    "#     os.makedirs(directory_name, exist_ok=True)\n",
    "#     dat = clean_dataframe(pd.read_csv(args.filename))\n",
    "#     # Save the dataframe\n",
    "#     dat.to_csv(directory_name + '/data_frame.csv')\n",
    "\n",
    "#     if args.save_dict:\n",
    "#         data_dict = prep_data(dat, subgroup_definitions[outcome_type][args.group])\n",
    "#         # Specify the filename for the gzipped pickle file\n",
    "#         dict_filename = 'births_{}_dict.pkl.gz'.format(args.group)\n",
    "\n",
    "#         # Open the file in binary write mode using gzip\n",
    "#         with gzip.open('{}/{}'.format(directory_name, dict_filename), 'wb') as f:\n",
    "#             # Use pickle to dump the data dictionary into the file\n",
    "#             pickle.dump(data_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/shaokangyang/Library/CloudStorage/GoogleDrive-sky.ang510@gmail.com/My Drive/Data/quarterly_fertility_mortality_031524.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(directory_name, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Clean data\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m dat \u001b[38;5;241m=\u001b[39m clean_dataframe(\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m, outcome_type\u001b[38;5;241m=\u001b[39moutcome_type, cat_name\u001b[38;5;241m=\u001b[39mgroup)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Save cleaned dataframe\u001b[39;00m\n\u001b[1;32m     24\u001b[0m dat\u001b[38;5;241m.\u001b[39mto_csv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory_name, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata_frame.csv\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/shaokangyang/Library/CloudStorage/GoogleDrive-sky.ang510@gmail.com/My Drive/Data/quarterly_fertility_mortality_031524.csv'"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Test code for interactive use\n",
    "    import pandas as pd\n",
    "    import gzip\n",
    "    import pickle\n",
    "    import os\n",
    "\n",
    "    # Set your input CSV path\n",
    "    filename = '/Users/shaokangyang/Library/CloudStorage/GoogleDrive-sky.ang510@gmail.com/My Drive/Data/quarterly_fertility_mortality_031524.csv'\n",
    "\n",
    "    # Configuration\n",
    "    save_dict = True\n",
    "    group = 'race'  # or 'age', 'edu', etc.\n",
    "    outcome_type = 'births'  # or 'deaths'\n",
    "\n",
    "    # Create output directory\n",
    "    directory_name = 'model_data/' + filename.split('/')[-1].split('.')[0]\n",
    "    os.makedirs(directory_name, exist_ok=True)\n",
    "\n",
    "    # Clean data\n",
    "    dat = clean_dataframe(pd.read_csv(filename), outcome_type=outcome_type, cat_name=group)\n",
    "\n",
    "    # Save cleaned dataframe\n",
    "    dat.to_csv(os.path.join(directory_name, 'data_frame.csv'))\n",
    "\n",
    "    # Optionally prepare and save dictionary\n",
    "    if save_dict:\n",
    "        data_dict = prep_data(dat, group=group, outcome_type=outcome_type)\n",
    "        dict_filename = f'births_{group}_dict.pkl.gz'\n",
    "        with gzip.open(os.path.join(directory_name, dict_filename), 'wb') as f:\n",
    "            pickle.dump(data_dict, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
